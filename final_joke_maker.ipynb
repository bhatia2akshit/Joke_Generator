{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Joke Maker\n",
    "### Use Mistral to extract keywords from joke"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load jokes into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true,
    "executionInfo": {
     "elapsed": 1630,
     "status": "ok",
     "timestamp": 1702225301542,
     "user": {
      "displayName": "akshit bhatia",
      "userId": "06692845535585475226"
     },
     "user_tz": -60
    },
    "id": "jVlFQmNU5l29",
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>joke</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A steak pun is a rare medium well done.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>They say that breakfast is the most important ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What do you get if you cross an angry sheep wi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                joke\n",
       "0            A steak pun is a rare medium well done.\n",
       "1  They say that breakfast is the most important ...\n",
       "2  What do you get if you cross an angry sheep wi..."
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('dad_jokes.csv',index_col=0)\n",
    "df[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Mistral LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2174b1d589e44901bfe32e69392f8207",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "check='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "model = AutoModelForCausalLM.from_pretrained(check,torch_dtype=torch.float16).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(check)\n",
    "tokenizer.add_special_tokens({'pad_token': '[PAD]'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "jokes_list = list(df['joke'][1:])\n",
    "def encode_sentences_batch(sentences, batch_size=16):\n",
    "    # Define prompt prefix and ending\n",
    "    prompt_prefix = '\\n<s>[INST] Instruction: Give only a list of topics that the given joke talks about in the same line? Do not give any explanation or suggestions after the answer. Joke: A steak pun is a rare medium well done.[/INST]\\nTopics: [steak, pun]<s>'\n",
    "    inst = '\\n[INST]Instruction: Give only a list of topics that the given joke talks about in the same line? Do not give any explanation or suggestions after the answer. Joke: '\n",
    "    ending = '[/INST]\\nTopics: '\n",
    "\n",
    "    # Tokenize and encode the prompts in batches\n",
    "    prompts = [prompt_prefix+ inst + sentence + ending for sentence in sentences]\n",
    "    return prompts\n",
    "\n",
    "prompts = encode_sentences_batch(jokes_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### For each prompt, generate keywords using Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a batch of prompts and then run the following code for each of these batches and save data along with processing of each batch.\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "batch_size=8\n",
    "counter=1\n",
    "joke_key_dict_list=[]\n",
    "\n",
    "for index, prompt in enumerate(prompts):\n",
    "    if counter%500==0:\n",
    "        #save result somewhere\n",
    "        df_save = pd.DataFrame(joke_key_dict_list)\n",
    "        df_save.to_csv('joke_keywords.csv', mode='a', index=False, header=False)\n",
    "        joke_key_dict_list = []\n",
    "        file2 = open('status.txt','a')\n",
    "        file2.write(f'{counter} out of {len(prompts)}\\n')\n",
    "        file2.close()\n",
    "\n",
    "    model_inputs = tokenizer(prompt, padding=True, truncation=True, return_tensors=\"pt\").to(\"cuda\")\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=20,)\n",
    "    output = tokenizer.batch_decode(generated_ids)[0]   \n",
    "    result = output.split('\\n')[-1].strip()\n",
    "\n",
    "    joke_key_dict={'joke':jokes_list[index],'keywords':result}\n",
    "    joke_key_dict_list.append(joke_key_dict)\n",
    "    counter+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the newly generated file full of keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Joke</th>\n",
       "      <th>Topics</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They say that breakfast is the most important ...</td>\n",
       "      <td>[breakfast, meal, day, poison, antidote]&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do you get if you cross an angry sheep wi...</td>\n",
       "      <td>Topics:  [angry sheep, moody cow, animal, baaa...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An apple a day keeps the doctor away. At least...</td>\n",
       "      <td>Topics:  [apple, doctor, away, throw]&lt;/s&gt;</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Joke  \\\n",
       "0  They say that breakfast is the most important ...   \n",
       "1  What do you get if you cross an angry sheep wi...   \n",
       "2  An apple a day keeps the doctor away. At least...   \n",
       "\n",
       "                                              Topics  \n",
       "0       [breakfast, meal, day, poison, antidote]</s>  \n",
       "1  Topics:  [angry sheep, moody cow, animal, baaa...  \n",
       "2          Topics:  [apple, doctor, away, throw]</s>  "
      ]
     },
     "execution_count": 184,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df_joke_words = pd.read_csv('joke_keywords.csv', names=['Joke','Topics'])\n",
    "df_joke_words[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_joke_words['Topics'] = df_joke_words['Topics'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(df_joke_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_list = []\n",
    "for element in df_joke_words.itertuples():\n",
    "    joke = element[1]\n",
    "    topics=element[2]\n",
    "        \n",
    "    if topics is None or not all(char in topics for char in (', ', '[')):\n",
    "        # df_list.append([element[1], element[2], ''])  # if comma and opening square bracket doesn't exist, then add nothing\n",
    "        continue\n",
    "    topics = element[2].split(', ')\n",
    "    topics_list=[]\n",
    "    topics_list.append(topics[0][topics[0].index('[')+1:])  # remove opening square bracket from the first element\n",
    "\n",
    "    if topics[-1].__contains__(']'):\n",
    "        topics_list.append(topics[-1][:topics[-1].index(']')])  # remove closing square bracket from the last element\n",
    "    else:\n",
    "        topics_list.append(topics[-1])\n",
    "    for topic in topics[1:-1]:  # iterate on the remaining elements\n",
    "        topics_list.append(topic.strip())\n",
    "    df_list.append([element[1], element[2], topics_list])  # add everything for the new dataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>They say that breakfast is the most important ...</td>\n",
       "      <td>[breakfast, meal, day, poison, antidote]&lt;/s&gt;</td>\n",
       "      <td>[breakfast, antidote, meal, day, poison]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What do you get if you cross an angry sheep wi...</td>\n",
       "      <td>Topics:  [angry sheep, moody cow, animal, baaa...</td>\n",
       "      <td>[angry sheep, baaad mooood, moody cow, animal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>An apple a day keeps the doctor away. At least...</td>\n",
       "      <td>Topics:  [apple, doctor, away, throw]&lt;/s&gt;</td>\n",
       "      <td>[apple, throw, doctor, away]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0  \\\n",
       "0  They say that breakfast is the most important ...   \n",
       "1  What do you get if you cross an angry sheep wi...   \n",
       "2  An apple a day keeps the doctor away. At least...   \n",
       "\n",
       "                                                   1  \\\n",
       "0       [breakfast, meal, day, poison, antidote]</s>   \n",
       "1  Topics:  [angry sheep, moody cow, animal, baaa...   \n",
       "2          Topics:  [apple, doctor, away, throw]</s>   \n",
       "\n",
       "                                                2  \n",
       "0        [breakfast, antidote, meal, day, poison]  \n",
       "1  [angry sheep, baaad mooood, moody cow, animal]  \n",
       "2                    [apple, throw, doctor, away]  "
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_list_new = pd.DataFrame(df_list)\n",
    "df_list_new[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning Mistral on this data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split df into test and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(df_list_new, test_size=0.2)\n",
    "\n",
    "# rename columns\n",
    "train.columns=['target','unwanted','text']\n",
    "test.columns=['target','unwanted','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_instruct(examples):\n",
    "    \n",
    "    inst = '<s><INST>Write a joke from the given list of topics</INST>\\nTopics: [angry sheep, baaad mooood, moody cow, animal]\\nJoke: What do you get if you cross an angry sheep with a moody cow? An animal that\\’s in a baaaaad mooood.<s>'\n",
    "    next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "    ending = 'Joke: '\n",
    "    new_prompts = [inst+'\\n'+next_inst+'['+', '.join(text)+']'+'\\n'+ending for text in examples]\n",
    "    return new_prompts\n",
    "    \n",
    "text_list_train = list(train['text'])\n",
    "text_list_test = list(test['text'])\n",
    "\n",
    "train_new = add_instruct(text_list_train)\n",
    "test_new = add_instruct(text_list_test)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>&lt;s&gt;&lt;INST&gt;Write a joke from the given list of t...</td>\n",
       "      <td>They say that breakfast is the most important ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>&lt;s&gt;&lt;INST&gt;Write a joke from the given list of t...</td>\n",
       "      <td>What do you get if you cross an angry sheep wi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>&lt;s&gt;&lt;INST&gt;Write a joke from the given list of t...</td>\n",
       "      <td>An apple a day keeps the doctor away. At least...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  <s><INST>Write a joke from the given list of t...   \n",
       "1  <s><INST>Write a joke from the given list of t...   \n",
       "2  <s><INST>Write a joke from the given list of t...   \n",
       "\n",
       "                                              target  \n",
       "0  They say that breakfast is the most important ...  \n",
       "1  What do you get if you cross an angry sheep wi...  \n",
       "2  An apple a day keeps the doctor away. At least...  "
      ]
     },
     "execution_count": 281,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.DataFrame(columns=['text','target'])\n",
    "df_train['text']=train_new\n",
    "df_train['target']=train['target']\n",
    "# df_test = pd.DataFrame(test_new,test['target'])\n",
    "df_train[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('./train.csv',index=0)\n",
    "df_test.to_csv('./test.csv',index=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'jokemaker'\n",
    "model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "push_to_hub = False\n",
    "# hf_token = \"YOUR HF TOKEN\"\n",
    "repo_id = \"abhatia/mistral_trained\"\n",
    "learning_rate = 2e-4\n",
    "num_epochs = 4\n",
    "batch_size = 1\n",
    "block_size = 1024\n",
    "trainer = \"sft\"\n",
    "warmup_ratio = 0.1\n",
    "weight_decay = 0.01\n",
    "gradient_accumulation = 4\n",
    "use_fp16 = True\n",
    "use_peft = True\n",
    "use_int4 = True\n",
    "lora_r = 16\n",
    "lora_alpha = 32\n",
    "lora_dropout = 0.045"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PROJECT_NAME\"] = project_name\n",
    "os.environ[\"MODEL_NAME\"] = model_name\n",
    "os.environ[\"PUSH_TO_HUB\"] = str(push_to_hub)\n",
    "# os.environ[\"HF_TOKEN\"] = hf_token\n",
    "os.environ[\"REPO_ID\"] = repo_id\n",
    "os.environ[\"LEARNING_RATE\"] = str(learning_rate)\n",
    "os.environ[\"NUM_EPOCHS\"] = str(num_epochs)\n",
    "os.environ[\"BATCH_SIZE\"] = str(batch_size)\n",
    "os.environ[\"BLOCK_SIZE\"] = str(block_size)\n",
    "os.environ[\"WARMUP_RATIO\"] = str(warmup_ratio)\n",
    "os.environ[\"WEIGHT_DECAY\"] = str(weight_decay)\n",
    "os.environ[\"GRADIENT_ACCUMULATION\"] = str(gradient_accumulation)\n",
    "os.environ[\"USE_FP16\"] = str(use_fp16)\n",
    "os.environ[\"USE_PEFT\"] = str(use_peft)\n",
    "os.environ[\"USE_INT4\"] = str(use_int4)\n",
    "os.environ[\"LORA_R\"] = str(lora_r)\n",
    "os.environ[\"LORA_ALPHA\"] = str(lora_alpha)\n",
    "os.environ[\"LORA_DROPOUT\"] = str(lora_dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[1mINFO    Running LLM\u001b[0m\n",
      "> \u001b[1mINFO    Params: Namespace(version=False, text_column='text', rejected_text_column='rejected', prompt_text_column='prompt', model_ref=None, warmup_ratio=0.1, optimizer='adamw_torch', scheduler='linear', weight_decay=0.01, max_grad_norm=1.0, add_eos_token=False, block_size=1024, peft=True, lora_r=16, lora_alpha=32, lora_dropout=0.045, logging_steps=-1, evaluation_strategy='epoch', save_total_limit=1, save_strategy='epoch', auto_find_batch_size=False, mixed_precision='fp16', quantization='int4', model_max_length=1024, trainer='default', target_modules=None, merge_adapter=True, use_flash_attention_2=False, dpo_beta=0.1, chat_template=None, padding=None, train=True, deploy=False, inference=False, username=None, backend='local-cli', token=None, repo_id=None, push_to_hub=False, model='mistralai/Mistral-7B-Instruct-v0.2', project_name='jokemaker', seed=42, epochs=4, gradient_accumulation=4, disable_gradient_checkpointing=False, lr=0.0002, log='none', data_path='data/', train_split='train', valid_split=None, batch_size=1, func=<function run_llm_command_factory at 0x7f406b09e5e0>)\u001b[0m\n",
      "> \u001b[1mINFO    Dataset: jokemaker (lm_training)\n",
      "Train data: ['data//train.csv']\n",
      "Valid data: []\n",
      "Column mapping: {'text': 'text', 'rejected_text': 'rejected', 'prompt': 'prompt'}\n",
      "\u001b[0m\n",
      "Saving the dataset (1/1 shards): 100%|█| 11880/11880 [00:00<00:00, 2002746.44 ex\n",
      "Saving the dataset (1/1 shards): 100%|█| 11880/11880 [00:00<00:00, 2256717.91 ex\n",
      "> \u001b[1mINFO    Starting local training...\u001b[0m\n",
      "> \u001b[1mINFO    {\"model\":\"mistralai/Mistral-7B-Instruct-v0.2\",\"project_name\":\"jokemaker\",\"data_path\":\"jokemaker/autotrain-data\",\"train_split\":\"train\",\"valid_split\":null,\"add_eos_token\":false,\"block_size\":1024,\"model_max_length\":1024,\"padding\":null,\"trainer\":\"default\",\"use_flash_attention_2\":false,\"log\":\"none\",\"disable_gradient_checkpointing\":false,\"logging_steps\":-1,\"evaluation_strategy\":\"epoch\",\"save_total_limit\":1,\"save_strategy\":\"epoch\",\"auto_find_batch_size\":false,\"mixed_precision\":\"fp16\",\"lr\":0.0002,\"epochs\":4,\"batch_size\":1,\"warmup_ratio\":0.1,\"gradient_accumulation\":4,\"optimizer\":\"adamw_torch\",\"scheduler\":\"linear\",\"weight_decay\":0.01,\"max_grad_norm\":1.0,\"seed\":42,\"chat_template\":null,\"quantization\":\"int4\",\"target_modules\":null,\"merge_adapter\":true,\"peft\":true,\"lora_r\":16,\"lora_alpha\":32,\"lora_dropout\":0.045,\"model_ref\":null,\"dpo_beta\":0.1,\"prompt_text_column\":\"autotrain_prompt\",\"text_column\":\"autotrain_text\",\"rejected_text_column\":\"autotrain_rejected_text\",\"push_to_hub\":false,\"repo_id\":null,\"username\":null,\"token\":null}\u001b[0m\n",
      "> \u001b[1mINFO    ['accelerate', 'launch', '--num_machines', '1', '--num_processes', '1', '--mixed_precision', 'fp16', '-m', 'autotrain.trainers.clm', '--training_config', 'jokemaker/training_params.json']\u001b[0m\n",
      "The following values were not passed to `accelerate launch` and had defaults used instead:\n",
      "\t`--dynamo_backend` was set to a value of `'no'`\n",
      "To avoid this warning pass in values for each of the problematic parameters or run `accelerate config`.\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m67\u001b[0m - \u001b[1mloading dataset from disk\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m108\u001b[0m - \u001b[1mTrain data: Dataset({\n",
      "    features: ['target', 'autotrain_text'],\n",
      "    num_rows: 11880\n",
      "})\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mprocess_input_data\u001b[0m:\u001b[36m109\u001b[0m - \u001b[1mValid data: None\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m197\u001b[0m - \u001b[1mcreating training arguments...\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m211\u001b[0m - \u001b[1mLogging steps: 25\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m252\u001b[0m - \u001b[1mloading model config...\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:26\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m264\u001b[0m - \u001b[1mloading model...\u001b[0m\n",
      "`low_cpu_mem_usage` was None, now set to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████████████| 3/3 [00:08<00:00,  2.89s/it]\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:36\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m332\u001b[0m - \u001b[1mmodel dtype: torch.float16\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:36\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m340\u001b[0m - \u001b[1mpreparing peft model...\u001b[0m\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:36\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m391\u001b[0m - \u001b[1mUsing block size 1024\u001b[0m\n",
      "Running tokenizer on train dataset: 100%|█| 11880/11880 [00:00<00:00, 12185.07 e\n",
      "Grouping texts in chunks of 1024 (num_proc=4): 100%|█| 11880/11880 [00:00<00:00,\n",
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/datasets/table.py:1395: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  block_group = [InMemoryTable(cls._concat_blocks(list(block_group), axis=axis))]\n",
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n",
      "\u001b[1m🚀 INFO  \u001b[0m | \u001b[32m2024-04-02 01:22:38\u001b[0m | \u001b[36m__main__\u001b[0m:\u001b[36mtrain\u001b[0m:\u001b[36m453\u001b[0m - \u001b[1mcreating trainer\u001b[0m\n",
      "/data/upb/users/b/bakshit/profiles/unix/cs/FraudDetectionThesis/venv/lib/python3.9/site-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
      "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
      "  warnings.warn(\n",
      "{'loss': 2.1083, 'grad_norm': 5.421453952789307, 'learning_rate': 3.565891472868217e-05, 'epoch': 0.08}\n",
      "{'loss': 1.0446, 'grad_norm': 0.7973151206970215, 'learning_rate': 7.131782945736435e-05, 'epoch': 0.16}\n",
      "{'loss': 0.6472, 'grad_norm': 3.4083685874938965, 'learning_rate': 0.00010852713178294573, 'epoch': 0.23}\n",
      "{'loss': 0.3329, 'grad_norm': 0.3918341100215912, 'learning_rate': 0.00014728682170542636, 'epoch': 0.31}\n",
      "{'loss': 0.2848, 'grad_norm': 0.4635538160800934, 'learning_rate': 0.000186046511627907, 'epoch': 0.39}\n",
      "{'loss': 0.2823, 'grad_norm': 0.4951269030570984, 'learning_rate': 0.00019723899913718724, 'epoch': 0.47}\n",
      "{'loss': 0.2681, 'grad_norm': 0.43697652220726013, 'learning_rate': 0.00019292493528904228, 'epoch': 0.54}\n",
      "{'loss': 0.2612, 'grad_norm': 0.4146484434604645, 'learning_rate': 0.00018861087144089733, 'epoch': 0.62}\n",
      "{'loss': 0.2519, 'grad_norm': 0.5552968382835388, 'learning_rate': 0.00018429680759275238, 'epoch': 0.7}\n",
      "{'loss': 0.2398, 'grad_norm': 0.6046363115310669, 'learning_rate': 0.00017998274374460743, 'epoch': 0.78}\n",
      "{'loss': 0.2325, 'grad_norm': 0.5217976570129395, 'learning_rate': 0.00017566867989646248, 'epoch': 0.85}\n",
      "{'loss': 0.2299, 'grad_norm': 0.5133095979690552, 'learning_rate': 0.00017135461604831753, 'epoch': 0.93}\n",
      "{'loss': 0.2227, 'grad_norm': 0.5049756765365601, 'learning_rate': 0.00016704055220017255, 'epoch': 1.01}\n",
      "{'loss': 0.1968, 'grad_norm': 0.5174869298934937, 'learning_rate': 0.00016272648835202763, 'epoch': 1.09}\n",
      "{'loss': 0.1948, 'grad_norm': 0.6186812520027161, 'learning_rate': 0.00015841242450388268, 'epoch': 1.16}\n",
      "{'loss': 0.187, 'grad_norm': 0.6636118292808533, 'learning_rate': 0.0001540983606557377, 'epoch': 1.24}\n",
      "{'loss': 0.1855, 'grad_norm': 0.6363934874534607, 'learning_rate': 0.00014978429680759277, 'epoch': 1.32}\n",
      "{'loss': 0.1848, 'grad_norm': 0.5508742332458496, 'learning_rate': 0.00014547023295944782, 'epoch': 1.4}\n",
      "{'loss': 0.1731, 'grad_norm': 0.5260041356086731, 'learning_rate': 0.00014115616911130284, 'epoch': 1.47}\n",
      "{'loss': 0.1737, 'grad_norm': 0.5553607940673828, 'learning_rate': 0.0001368421052631579, 'epoch': 1.55}\n",
      "{'loss': 0.1712, 'grad_norm': 0.4714481234550476, 'learning_rate': 0.00013252804141501294, 'epoch': 1.63}\n",
      "{'loss': 0.1641, 'grad_norm': 0.5009799599647522, 'learning_rate': 0.000128213977566868, 'epoch': 1.71}\n",
      "{'loss': 0.1636, 'grad_norm': 0.512673020362854, 'learning_rate': 0.00012389991371872304, 'epoch': 1.78}\n",
      "{'loss': 0.1639, 'grad_norm': 0.5506978631019592, 'learning_rate': 0.00011958584987057809, 'epoch': 1.86}\n",
      "{'loss': 0.1598, 'grad_norm': 0.4989047348499298, 'learning_rate': 0.00011527178602243314, 'epoch': 1.94}\n",
      "{'loss': 0.1517, 'grad_norm': 0.5140470862388611, 'learning_rate': 0.00011095772217428819, 'epoch': 2.02}\n",
      "{'loss': 0.144, 'grad_norm': 0.5103261470794678, 'learning_rate': 0.00010664365832614322, 'epoch': 2.09}\n",
      "{'loss': 0.1427, 'grad_norm': 0.5491327047348022, 'learning_rate': 0.00010232959447799828, 'epoch': 2.17}\n",
      "{'loss': 0.1403, 'grad_norm': 0.43741852045059204, 'learning_rate': 9.801553062985332e-05, 'epoch': 2.25}\n",
      "{'loss': 0.1364, 'grad_norm': 0.5243673920631409, 'learning_rate': 9.370146678170837e-05, 'epoch': 2.33}\n",
      "{'loss': 0.1319, 'grad_norm': 0.5339713096618652, 'learning_rate': 8.938740293356343e-05, 'epoch': 2.4}\n",
      "{'loss': 0.1353, 'grad_norm': 0.5762388110160828, 'learning_rate': 8.507333908541847e-05, 'epoch': 2.48}\n",
      "{'loss': 0.1329, 'grad_norm': 0.44508349895477295, 'learning_rate': 8.075927523727351e-05, 'epoch': 2.56}\n",
      "{'loss': 0.1432, 'grad_norm': 0.5540634989738464, 'learning_rate': 7.644521138912856e-05, 'epoch': 2.64}\n",
      "{'loss': 0.1387, 'grad_norm': 0.41015195846557617, 'learning_rate': 7.213114754098361e-05, 'epoch': 2.71}\n",
      "{'loss': 0.1347, 'grad_norm': 0.4499039649963379, 'learning_rate': 6.781708369283866e-05, 'epoch': 2.79}\n",
      "{'loss': 0.1361, 'grad_norm': 0.4912538230419159, 'learning_rate': 6.35030198446937e-05, 'epoch': 2.87}\n",
      "{'loss': 0.136, 'grad_norm': 0.48605841398239136, 'learning_rate': 5.918895599654876e-05, 'epoch': 2.95}\n",
      "{'loss': 0.1296, 'grad_norm': 0.533596396446228, 'learning_rate': 5.48748921484038e-05, 'epoch': 3.02}\n",
      "{'loss': 0.1212, 'grad_norm': 0.5283880829811096, 'learning_rate': 5.056082830025884e-05, 'epoch': 3.1}\n",
      "{'loss': 0.12, 'grad_norm': 0.48971110582351685, 'learning_rate': 4.624676445211389e-05, 'epoch': 3.18}\n",
      "{'loss': 0.1168, 'grad_norm': 0.4500426948070526, 'learning_rate': 4.193270060396894e-05, 'epoch': 3.26}\n",
      "{'loss': 0.1174, 'grad_norm': 0.5299525856971741, 'learning_rate': 3.761863675582399e-05, 'epoch': 3.33}\n",
      "{'loss': 0.1182, 'grad_norm': 0.428524911403656, 'learning_rate': 3.330457290767903e-05, 'epoch': 3.41}\n",
      "{'loss': 0.1176, 'grad_norm': 0.515098512172699, 'learning_rate': 2.899050905953408e-05, 'epoch': 3.49}\n",
      "{'loss': 0.1165, 'grad_norm': 0.43362826108932495, 'learning_rate': 2.467644521138913e-05, 'epoch': 3.57}\n",
      " 91%|███████████████████████████████████▌   | 1174/1288 [33:27<03:13,  1.70s/it]"
     ]
    }
   ],
   "source": [
    "!autotrain llm \\\n",
    "--train \\\n",
    "--model ${MODEL_NAME} \\\n",
    "--project-name ${PROJECT_NAME} \\\n",
    "--data-path data/ \\\n",
    "--text-column text \\\n",
    "--lr ${LEARNING_RATE} \\\n",
    "--batch-size ${BATCH_SIZE} \\\n",
    "--merge_adapter \\\n",
    "--epochs ${NUM_EPOCHS} \\\n",
    "--block-size ${BLOCK_SIZE} \\\n",
    "--warmup-ratio ${WARMUP_RATIO} \\\n",
    "--lora-r ${LORA_R} \\\n",
    "--lora-alpha ${LORA_ALPHA} \\\n",
    "--lora-dropout ${LORA_DROPOUT} \\\n",
    "--weight-decay ${WEIGHT_DECAY} \\\n",
    "--gradient-accumulation ${GRADIENT_ACCUMULATION} \\\n",
    "$( [[ \"$USE_FP16\" == \"True\" ]] && echo \"--mixed-precision fp16\" ) \\\n",
    "$( [[ \"$USE_INT4\" == \"True\" ]] && echo \"--quantization int4\" ) \\\n",
    "$( [[ \"$USE_PEFT\" == \"True\" ]] && echo \"--use-peft\" ) \\\n",
    "$( [[ \"$PUSH_TO_HUB\" == \"True\" ]] && echo \"--push-to-hub --token ${HF_TOKEN} --repo-id ${REPO_ID}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference Time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fae767fb1cdb4658a1c5a6488581dbdb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "adapter_model ='firstmodel'\n",
    "base_model_name = 'mistralai/Mistral-7B-Instruct-v0.2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(adapter_model)\n",
    "model = AutoModelForCausalLM.from_pretrained(base_model_name)\n",
    "model = PeftModel.from_pretrained(model, adapter_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "prompt='[hot girls, pollution, sky]'\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20,)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "inputs = tokenizer(\"[hot girls, pollution, sky]\", return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13bdd45bd78a4114bf653ab349bf8e21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "check='jokemaker'\n",
    "model_custom = AutoModelForCausalLM.from_pretrained(check,torch_dtype=torch.float16).to('cuda')\n",
    "tokenizer_custom = AutoTokenizer.from_pretrained(check)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joke for topics: [hot girls, pollution, sky]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <INST>Write a joke from the given list of topics</INST>\\nTopics: [hot girls, pollution, sky]\\nJoke: \\nWhy did the hot girl go to the junkyard?\\nTo find a sky-diver'"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finetuned Mistral\n",
    "next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "prompt=next_inst+'[hot girls, pollution, sky]\\nJoke: '\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=20,)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <INST>Write a joke from the given list of topics</INST>\\nTopics: [hot girls, pollution, sky]\\nJoke: \\nWhy did the hot girl carry an umbrella in the clear sky?\\nBecause she was following the instructions on the bottle of pollution! \\n(Note: This joke is meant to be a commentary on the sad reality of pollution and how it affects everyone, even the most beautiful among us.)</s>'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General Mistral\n",
    "next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "prompt=next_inst+'[hot girls, pollution, sky]\\nJoke: '\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100,)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joke for topics: [students, classroom, food, teacher, strict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> <INST>Write a joke from the given list of topics</INST>\\nTopics: [students, classroom, food, teacher, strict]\\nJoke: \\nWhy did the teacher tell her student to eat his homework?\\nBecause it was a piece of cake, and she knew he couldn't 'bake' it. (Replace 'it' with 'that')</s>\""
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finetuned Mistral\n",
    "next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "prompt=next_inst+'[students, classroom, food, teacher, strict]\\nJoke: '\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100,temperature=0.6, do_sample=True)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> <INST>Write a joke from the given list of topics</INST>\\nTopics: [students, classroom, food, teacher, strict]\\nJoke: \\nWhy don't teachers ever go hungry at lunchtime in a strict classroom?\\nBecause their students are always bringing them sandwiches! (get it, bringing, like they're allowed to bring things to class but in a classroom that's strict, it's usually not allowed) \\n\\nI hope you found that joke amusing! Let me know if you need help with anything else. 😊</s>\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General Mistral Performance\n",
    "next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "prompt=next_inst+'[students, classroom, food, teacher, strict]\\nJoke: '\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100,temperature=0.6, do_sample=True)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joke for topics: [man, dogs, cats, hot girls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <INST>Write a joke from the given list of topics</INST>\\nTopics: [man, dogs, cats, hot girls]\\nJoke: \\nWhy did the man name his dog \"Startled\" ?\\nBecause it was a \\'Shock\\' to see him with a \\'Hot\\' girl and a \\'Barking\\' dog!</s>'"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Finetuned Mistral\n",
    "next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "prompt=next_inst+'[man, dogs, cats, hot girls]\\nJoke: '\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100,temperature=0.6, do_sample=True)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'<s> <INST>Write a joke from the given list of topics</INST>\\nTopics: [man, dogs, cats, hot girls]\\nJoke: \\nWhy did the man name his dog \"Hot Stuff\"?\\nBecause he wanted a little \"bark\" in his life, but when he got home, he found his cat had named his new kitten \"Hotter Stuff\"!\\n\\nSo now the man had a dilemma, two pets with the same name! He decided to call the dog \"Hot Rod\" instead, and the cat\\'s kitten became \"Hot Tuna\". From that day on, the'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## General Mistal\n",
    "\n",
    "next_inst = '<INST>Write a joke from the given list of topics</INST>\\nTopics: '\n",
    "prompt=next_inst+'[man, dogs, cats, hot girls]\\nJoke: '\n",
    "model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "generated_ids = model.generate(**model_inputs, max_new_tokens=100,temperature=0.6, do_sample=True)\n",
    "output = tokenizer.batch_decode(generated_ids)[0]\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Jokes by General Mistral"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "627306c9a1f3480eb691f838d8c795a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "check='mistralai/Mistral-7B-Instruct-v0.2'\n",
    "model = AutoModelForCausalLM.from_pretrained(check,torch_dtype=torch.float16).to('cuda')\n",
    "tokenizer = AutoTokenizer.from_pretrained(check)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOi7JjYIIdX8oFuZJzDgpI/",
   "gpuType": "V100",
   "machine_shape": "hm",
   "mount_file_id": "1pWs1xjSKJmpLwcR7fN8oXTuI4Ybs0SYF",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "06707a929e5d40d3914945c63e78ea2f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea02d3ed08854ce3b661f3b1f06cbe97",
      "placeholder": "​",
      "style": "IPY_MODEL_b2b9d4a6c32042cb957c815116a37e25",
      "value": " 5115/10000 [1:17:07&lt;1:16:04,  1.07it/s]"
     }
    },
    "0e34510a35144240b71089ee2610f82b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_72d5b010fbea4232ae15e9b5a4eb8c46",
       "IPY_MODEL_d3f2534159fb4824bb7289ce57a9def9",
       "IPY_MODEL_3292ae52157646599051749c5c227b64"
      ],
      "layout": "IPY_MODEL_1cc4f3155ea74cef8d6e6b12d2ed05d9"
     }
    },
    "1cc4f3155ea74cef8d6e6b12d2ed05d9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2c5ffc98516c4663b8522ecadb9e1f59": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "30c11ab2cb634d80ada25ce535a65316": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3292ae52157646599051749c5c227b64": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_30c11ab2cb634d80ada25ce535a65316",
      "placeholder": "​",
      "style": "IPY_MODEL_d1e540da631147a3bcc41ff16e671d38",
      "value": " 10000/10000 [00:42&lt;00:00, 276.73it/s]"
     }
    },
    "371715200a004c4b8b759949f87c7a2a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "580e1458025549a69f9cb056573c8309": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "583aa23d66e74a36aa73097695bbb776": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "5b80e2a485dc42f7b22291cec35a54b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "6f58e312cb0a44d18a0dc0f940e7b72b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7a22718ec1b2468294a0fb420d8ca5e5",
      "placeholder": "​",
      "style": "IPY_MODEL_bb74585210504a8ca7394a395195deed",
      "value": " 51%"
     }
    },
    "72d5b010fbea4232ae15e9b5a4eb8c46": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_371715200a004c4b8b759949f87c7a2a",
      "placeholder": "​",
      "style": "IPY_MODEL_5b80e2a485dc42f7b22291cec35a54b0",
      "value": "100%"
     }
    },
    "7a22718ec1b2468294a0fb420d8ca5e5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "890ed31102da45929cde0f4ad5850e8d": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b2b9d4a6c32042cb957c815116a37e25": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b575bcbf75154393944ce25cb9a612ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f58e312cb0a44d18a0dc0f940e7b72b",
       "IPY_MODEL_fcba867e7720448698363fd6b58febf4",
       "IPY_MODEL_06707a929e5d40d3914945c63e78ea2f"
      ],
      "layout": "IPY_MODEL_e74a300095d945c49dfabb30c5911275"
     }
    },
    "bb74585210504a8ca7394a395195deed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d1e540da631147a3bcc41ff16e671d38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "d3f2534159fb4824bb7289ce57a9def9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c5ffc98516c4663b8522ecadb9e1f59",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_580e1458025549a69f9cb056573c8309",
      "value": 10000
     }
    },
    "e74a300095d945c49dfabb30c5911275": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea02d3ed08854ce3b661f3b1f06cbe97": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fcba867e7720448698363fd6b58febf4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_890ed31102da45929cde0f4ad5850e8d",
      "max": 10000,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_583aa23d66e74a36aa73097695bbb776",
      "value": 5115
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
